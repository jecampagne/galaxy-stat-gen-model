\documentclass[11pt]{amsart}
\usepackage{amsaddr}
\usepackage{mathtools}
\usepackage{aas_macros} % macro pour Bibtex
\usepackage{hyperref}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsthm}
\usepackage{amsmath}
% \usepackage{makecell}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{color}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage{natbib,fancyhdr} %new
\usepackage{bm}

%\usepackage{mathtools} % psmallmatrix

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\setlength{\heavyrulewidth}{3\lightrulewidth}
\setlength{\abovetopsep}{1ex}

\newcommand{\Esp}[0]{\ensuremath{\mathbb{E}}}
\newcommand{\DKL}[0]{\ensuremath{\mathbb{D}_{\mathsf{KL}}}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\usepackage[top=3cm, bottom=2cm, left=3cm, right=2cm]{geometry} %margins

% \usepackage{geometry}
%  \geometry{
%  a4paper,
%  left=25mm,
%  }

\newcommand{\jessa}[1]{{\color{red} #1}}
\newcommand{\field}[1]{\mathbf{#1}}
\newcommand{\nn}{\nonumber}


% My definitions:
\def\U{\mathcal U}
\def\LL{\mathcal L}
\def\P{\mathcal P}
\def\D{\mathcal D}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% My packages:
\usepackage{algorithmic}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{xcolor}
\usepackage{multicol} 
\usepackage{enumitem}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\newcommand{\bl}[1]{{\color{blue} #1}}
\newcommand{\rd}[1]{{\color{red} #1}}


% Alternative Assumption!

\newtheorem{theorem}{Theorem}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}{Definition}

% New environment
\newenvironment{assumptionp}[1]{
  \renewcommand\theassumptionalt{#1}
  \assumptionalt
}{\endassumptionalt}

\graphicspath{{./figures}}
\usepackage[margin=1cm]{caption}



\title{Galaxy image statistical generator: how to be confident?}
\author{Jean-Eric Campagne}
\address{Université Paris-Saclay, CNRS/IN2P3, IJCLab, 91405 Orsay, France
}
\email{jean-eric.campagne@ijclab.in2p3.fr}
\date{\today}

\begin{document}
\maketitle
%\renewcommand{\baselinestretch}{0.75}\normalsize
%\tableofcontents
%\renewcommand{\baselinestretch}{1.0}\normalsize
%
\begin{abstract}
blabla
\\
\smallskip
\noindent \textbf{Keywords.} diffusion models, U-Net
\end{abstract}

\section{Introduction}
\label{sec:Intro}
Image generation in Machine Learning is a challenging task that has made a dramatic rise in quality recently thanks to the large scale statistical model architectures as for instance \texttt{DALL-E 2} \citep{ramesh2022}, \texttt{Midjourney} \citep{Oppenlaender2022} and \texttt{StableDiffusion} \citep{Rombach2022} which use \textit{stochastic diffusion processes}. Such models have replaced the previous generation based on \textit{variational auto encoder} (aka VAE) \citep{Kingma2014} and  \textit{adversarial networks} (aka GAN) \citep{goodfellow2014generative} as in \citep{KarrasALL18}, or \textit{normalizing flows} (hereafter nicknamed NF) such as in \texttt{Glow} \citep{Kingma2018}. 


The ability of generative models have been rapidly adopted in many domains. For instance, in High Energy Physics the reader may be interested by the review \cite{PhysRevD.107.076017}. Concerning Astrophysics and Cosmology the generative models are used in different tasks (eg. deblending \citep{Hemmati_2022,Arcelin2020} {\color{red} à compléter}) and especially to create synthetic galaxy images of complex morphologies going beyond parametrized analytic light profile simulations (eg. \texttt{GalSim} by \cite{ROWE2015121}). Along this line \citep{ravanbakhsh2016,Fussell2019} use GAN models, \cite{Lanusse2021} propose an hybrid VAE-Normalizing Flow architecture and \cite{smith2021} use a denoising stochastic diffusion model (hereafter nicknamed DSDM). 

Despite the impressive image quality of these generative models that are nowadays well diffused in the general public, many questions arise from mathematical and usage point of views. Notably, one may ask for what is really learned by the generative models and what are the statistical properties of the generated samples. In a general manner \cite{Hataya2023} ask if the image generated can corrupt the future datasets. However, the study is devoted  to the large scale statistical models mentioned above that are trained usually with billion-scale data extracted from the Internet, and so in turn can be contaminated by the images shared by many users. Concerning galaxy image generation the datasets are much more modest for the time being (i.e $O(10^5)$) collected from optical surveys (eg. COSMOS HST Advanced Camera for Surveys, \cite{mandelbaum_2019_3242143},  Sloan Digital Sky Survey \citep[SDSS;][]{sdss} Data Release 7 \citep{sdssdr7}) and used by a rather small community compared to social network followers which certainly  limits the datasets corruption. So, besides image corruption, \cite{HACKSTEIN2023100685,janulewicz2024assessing} have investigated different metrics to address the question of fidelity of the generated galaxy images and morphological properties compared to the original dataset.

\cite{kadkhodaie2024generalization} address more mathematically oriented questions and demonstrate the passage between memorization to generalization of  diffusion generative models when the dataset size increases and propose interpretation of what is learned by the network as a sort of \textit{geometry-adaptive harmonic basis} going beyond the commonly used \textit{steerable wavelet basis} first introduced and studied in \cite{Simoncelli1995b,Unser2013} and developped for instance in the \texttt{Kyamato} library \citep{JMLR:v21:19-047}. The authors uses \textit{denoiser} architectures as \texttt{UNet} \citep{2015arXiv150504597R} and \texttt{BF-CNN} networks \citep{Mohan2020Robust} trained on \texttt{CelebA} \citep{Liu2015} and \texttt{LSUN} bedroom  \citep{Yu2015} reduced datasets of $O(10^5)$ images resized to $80\times 80$ pixels (grayscale). The dataset size and the number of pisels per image make possible to investigate how the \texttt{Glow} normalizing flow based model compare to denoiser based diffusion model concerning the generation of galaxy images both trained with the same dataset issued from DR7 SDSS survey.


In the following sections, first we briefly describe the different kind of generative models and we show the outcome of the conducted numerical experiment before drawing some guide lines. Appendix gives more details on the dataset used.
%
\section{Generative Models}
%
The generative models assume that the observations $\bm{x}$ are described by a probability distribution $p(\bm{x})$ (\textit{pdf}) and aim to give a representation and leads to different strategies.
%
\subsection{Variational Auto-Encoder}
%
Using \texttt{VAE} means that it is assumed that there exists an underlying unobservable stochastic variable $\bm{z}$  (\textit{latent variable}) whose \textit{a priori} known $\pi(\bm{z})$ \textit{pdf} such that $p(\bm{x}) = p(\bm{x}|\bm{z})\pi(\bm{z})$. Although, the likelihood  $p(\bm{x}|\bm{z})$ is unknown and one introduces a parametrized version $p_{\bm{\theta}}(\bm{x}|\bm{z})$ and the parameters $\bm{\theta}$ might be adjusted in such a way that  $p_{\bm{\theta}}(x)$ matches the empirical distribution of the $n$ data samples ($\{\bm{x}^{i}\}_{i\leq n}$). The best solution $\bm{\theta}_{ML}$ is the maximum of $p_{\bm{\theta}}(x)$ but in practice this leads to  intractable problem (analytical or numerical) as one should evaluate the gradient with respect to $\bm{\theta}$ of the integral 
$\int\ p_{\bm{\theta}}(\bm{x}|\bm{z})\pi(\bm{z}) dz$. To overcome such problem, one uses first the Bayes relation
\begin{equation}
p_{\bm{\theta}}(x) p(\bm{z}|\bm{x}) = p_{\bm{\theta}}(\bm{x}|\bm{z}) \pi(\bm{z})
\end{equation}
and second a parametrized version of the unknown true \textit{a posteriori} distribution as $p_{\bm{\phi}}(\bm{z}|\bm{x})$. If one introduces the Kullback-Leibler divergence $\DKL(p\| q)=\Esp_{x\sim p}[\log(p(x)/q(x))]$, then taking the expected value according to $\bm{z}\sim p_{\bm{\phi}}(\bm{z}|\bm{x})$ of the both sides of the following expression
\begin{align}
\log p(\bm{x}) = \log p_{\bm{\theta},\bm{\phi}}(x) &= \log p_{\bm{\theta}}(\bm{x}|\bm{z}) + \log \pi(\bm{z}) - \log p(\bm{z}|\bm{x}) + \log p_{\bm{\phi}}(\bm{z}|\bm{x}) - \log p_{\bm{\phi}}(\bm{z}|\bm{x})
\end{align}
yields 
\begin{align}
\log p(\bm{x})  &= \Esp_{z\sim p_{\bm{\phi}}}[\log p_{\bm{\theta}}(\bm{x}|\bm{z})] - \DKL(p_{\bm{\phi}}(\bm{z}|\bm{x})\| \pi(\bm{z})) + \DKL(p_{\bm{\phi}}(\bm{z}|\bm{x})\| p(\bm{z}|\bm{x}))
&\geq \matcal{L}(\bm{x};\{\bm{\theta},\bm{\phi}\})
\end{align}
where we have emphasize the \textit{evidence lower bound} (aka ELBO)
\begin{equation}
\matcal{L}(\bm{x};\{\bm{\theta},\bm{\phi}\}) = \Esp_{z\sim p_{\bm{\phi}}}[\log p_{\bm{\theta}}(\bm{x}|\bm{z})] - \DKL(p_{\bm{\phi}}(\bm{z}|\bm{x})\| \pi(\bm{z}))
\end{equation}
and use the positiveness property of the $\DKL$ divergence. Maximizing with respect to 
$\{\bm{\theta},\bm{\phi}\}$ the ELBO is now tractable as it is free of the unknown distributions $p(\bm{x})$ and $p(\bm{z}|\bm{x})$.

It is usual to call $p_{\bm{\theta}}(\bm{x}|\bm{z})$ the \textit{decoder} part of the whole model while $p_{\bm{\phi}}(\bm{z}|\bm{x})$ is the \textit{encoder} part. Once the optimization is done, then decoder is used as a $\bm{x}$ sample generator when it is fed with $\bm{z}$ samples according to $\bm{z}\sim\pi(\bm{z})$.



%
\section{Experiment}

%\begin{figure}
%\centering
%\includegraphics[width=0.4\columnwidth]{filtreDb4.png}
%\caption{Coverage of the Fourier domain without gap by a low-pass filter and a collection of band-pass filters.}
%\label{fig-Daubechies-db4}
%\end{figure}


\section{Discussion}


\section*{Acknowledgements}

\section*{Codes}

%%%%%%%%%%
\addcontentsline{toc}{section}{References}
% Put your bibiliography file here
%\section{Bibliography}
\bibliographystyle{elsarticle-harv}%{aa}
\bibliography{refs.bib}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\section{Details on the dataset}


\end{document}
